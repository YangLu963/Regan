import torch
import torch.optim as optim
import numpy as np
import yaml
import os
from collections import deque
import time
import warnings
warnings.filterwarnings('ignore')

from ragen.qwen_agent import QwenRAGENAgent
from ragen.experience_buffer import ExperienceBuffer
from ragen.apo_trainer import APOTrainer
from ragen.webshop_env import WebShopEnv
from ragen.reward_calculator import RewardCalculator

class RAGENWebShopTrainer:
    def __init__(self, config_path="configs/webshop_config.yaml"):
        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        print("=" * 60)
        print("RAGEN + A*PO + Qwen WebShop è®­ç»ƒç³»ç»Ÿ")
        print("=" * 60)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.env = WebShopEnv(
            server_url=self.config['environment']['server_url'],
            max_steps=self.config['environment']['max_steps']
        )
        
        self.agent = QwenRAGENAgent(
            model_name=self.config['model']['base_model'],
            device=self.config['model']['device']
        )
        
        # å‚è€ƒç­–ç•¥ï¼ˆå›ºå®šï¼‰
        self.reference_agent = QwenRAGENAgent(
            model_name=self.config['model']['base_model'],
            device=self.config['model']['device']
        )
        
        self.reward_calculator = RewardCalculator()
        self.optimizer = optim.Adam(self.agent.parameters(), lr=self.config['training']['learning_rate'])
        self.buffer = ExperienceBuffer(self.config['buffer']['capacity'])
        self.apo_trainer = APOTrainer(
            beta=self.config['training']['beta'],
            gamma=self.config['training']['gamma'],
            cache_file=self.config['vstar_cache']['cache_file'],
            num_vstar_samples=self.config['vstar_cache']['num_vstar_samples']
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = deque(maxlen=20)
        self.success_rates = deque(maxlen=20)
        self.format_success_rates = deque(maxlen=20)  # æ ¼å¼æˆåŠŸç‡
        self.best_success_rate = 0.0
        self.total_steps = 0
        
    def collect_experience(self, num_episodes=2):
        """æ”¶é›†ç»éªŒæ•°æ®"""
        print(f"\nğŸ“¥ æ”¶é›† {num_episodes} ä¸ªå›åˆçš„ç»éªŒ...")
        
        for episode in range(num_episodes):
            obs, info = self.env.reset()
            instruction = info['instruction']
            episode_reward = 0
            done = False
            steps = 0
            
            print(f"\n--- å›åˆ {episode+1} ---")
            print(f"ä»»åŠ¡: {instruction}")
            
            while not done and steps < self.config['environment']['max_steps']:
                # Qwenç”Ÿæˆæ€è€ƒå’ŒåŠ¨ä½œ
                think_content, action_content, log_prob, full_response = self.agent.generate_webshop_response(obs, instruction)
                
                print(f"\næ­¥éª¤ {steps+1}:")
                print(f"æ€è€ƒ: {think_content[:100]}...")
                print(f"åŠ¨ä½œ: {action_content}")
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_obs, env_reward, done, info = self.env.step(action_content, info['session_id'])
                
                # è®¡ç®—è¯¦ç»†å¥–åŠ±
                task_success = (env_reward > 0.5)
                reward = self.reward_calculator.calculate_reward(think_content, action_content, next_obs, task_success)
                
                episode_reward += reward
                steps += 1
                self.total_steps += 1
                
                # å­˜å‚¨ç»éªŒ
                self.buffer.push(obs, instruction, think_content, action_content, reward, done, log_prob)
                
                obs = next_obs
                
                if done:
                    break
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            self.episode_rewards.append(episode_reward)
            success = 1 if episode_reward > 0.5 else 0
            self.success_rates.append(success)
            
            # æ ¼å¼æˆåŠŸç‡ï¼ˆå…³é”®æŒ‡æ ‡ï¼‰
            format_success = 1 if self._check_format_success(think_content, action_content) else 0
            self.format_success_rates.append(format_success)
            
            current_success = np.mean(self.success_rates) if self.success_rates else 0
            current_format_success = np.mean(self.format_success_rates) if self.format_success_rates else 0
            
            print(f"\nå›åˆç»“æœ: æ€»å¥–åŠ±={episode_reward:.2f}, æˆåŠŸç‡={current_success:.3f}, æ ¼å¼æˆåŠŸç‡={current_format_success:.3f}")
    
    def _check_format_success(self, think_content, action_content):
        """æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®"""
        return (think_content and "<think>" in think_content and "</think>" in think_content and
                action_content and "<action>" in action_content and "</action>" in action_content)
    
    def train_step(self):
        """æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤"""
        if len(self.buffer) < self.config['training']['batch_size']:
            return None
            
        batch = self.buffer.sample(self.config['training']['batch_size'])
        if batch is None:
            return None
        
        # è®¡ç®—A*POä¼˜åŠ¿
        advantages, v_star_values = self.apo_trainer.compute_advantages(
            batch['observations'], batch['rewards'], batch['dones'],
            self.reference_agent, self.agent
        )
        
        # è®¡ç®—å‚è€ƒç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
        with torch.no_grad():
            ref_log_probs = []
            for (obs, instruction) in batch['observations']:
                _, _, ref_log_prob, _ = self.reference_agent.generate_webshop_response(obs, instruction)
                ref_log_probs.append(ref_log_prob)
            ref_log_probs = torch.FloatTensor(ref_log_probs)
        
        # è®¡ç®—A*POç­–ç•¥æŸå¤±
        policy_loss, pg_loss, kl_penalty = self.apo_trainer.compute_policy_loss(
            batch['log_probs'], advantages, ref_log_probs
        )
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        policy_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.agent.parameters(), self.config['training']['grad_clip'])
        self.optimizer.step()
        
        return {
            'total_loss': policy_loss.item(),
            'policy_loss': pg_loss,
            'kl_penalty': kl_penalty,
            'avg_advantage': advantages.mean().item()
        }
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
        print("æˆåŠŸæ ‡å‡†: æˆåŠŸç‡ä»0%æå‡åˆ°20%+")
        print("é‡ç‚¹è§‚å¯Ÿ: Base Modelå­¦ä¹ æ ¼å¼éµå¾ªèƒ½åŠ›")
        print("-" * 50)
        
        start_time = time.time()
        
        for epoch in range(self.config['training']['num_epochs']):
            # é˜¶æ®µ1: æ”¶é›†ç»éªŒ
            self.collect_experience(num_episodes=2)
            
            # é˜¶æ®µ2: è®­ç»ƒ
            if len(self.buffer) >= self.config['training']['batch_size']:
                loss_info = self.train_step()
                
                if loss_info and epoch % 5 == 0:
                    current_success = np.mean(self.success_rates) if self.success_rates else 0
                    current_format = np.mean(self.format_success_rates) if self.format_success_rates else 0
                    
                    print(f"Epoch {epoch:3d} | Loss: {loss_info['total_loss']:7.4f} | "
                          f"Success: {current_success:5.3f} | Format: {current_format:5.3f} | "
                          f"Buffer: {len(self.buffer):2d}")
            
            # é˜¶æ®µ3: è¯„ä¼°å’Œæ£€æŸ¥åœæ­¢æ¡ä»¶
            if epoch % 10 == 0:
                current_success = np.mean(self.success_rates) if self.success_rates else 0
                current_format = np.mean(self.format_success_rates) if self.format_success_rates else 0
                training_time = (time.time() - start_time) / 60
                
                if current_success > self.best_success_rate:
                    self.best_success_rate = current_success
                
                print(f"\n=== è¯„ä¼° Epoch {epoch} ===")
                print(f"è®­ç»ƒæ—¶é—´: {training_time:6.1f} åˆ†é’Ÿ")
                print(f"æ€»æ­¥æ•°: {self.total_steps:6d}")
                print(f"å½“å‰æˆåŠŸç‡: {current_success:6.3f}")
                print(f"æ ¼å¼æˆåŠŸç‡: {current_format:6.3f}")
                print(f"å†å²æœ€ä½³: {self.best_success_rate:6.3f}")
                
                # æˆåŠŸæ ‡å‡†æ£€æŸ¥
                if current_success >= 0.20:
                    print("ğŸ‰" * 20)
                    print("è¾¾åˆ°Part 2ä½œä¸šè¦æ±‚: æˆåŠŸç‡ > 20%!")
                    print("Base ModelæˆåŠŸå­¦ä¹ äº†æ ¼å¼éµå¾ªå’Œä»»åŠ¡è§£å†³!")
                    print("å¯ä»¥åœæ­¢è®­ç»ƒå¹¶å‡†å¤‡æ¼”ç¤º")
                    print("ğŸ‰" * 20)
                    break
                    
                print("-" * 40)
        
        # æœ€ç»ˆç»Ÿè®¡
        total_time = (time.time() - start_time) / 60
        final_success = np.mean(self.success_rates) if self.success_rates else 0
        final_format = np.mean(self.format_success_rates) if self.format_success_rates else 0
        
        print(f"\n" + "=" * 50)
        print("è®­ç»ƒå®Œæˆ!")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f} åˆ†é’Ÿ")
        print(f"æœ€ç»ˆæˆåŠŸç‡: {final_success:.3f}")
        print(f"æœ€ç»ˆæ ¼å¼æˆåŠŸç‡: {final_format:.3f}")
        print(f"å†å²æœ€ä½³æˆåŠŸç‡: {self.best_success_rate:.3f}")
        print(f"æ€»è®­ç»ƒæ­¥æ•°: {self.total_steps}")
        print("=" * 50)
        
        self.env.close()

def main():
    # åˆ›å»ºç›®å½•
    os.makedirs("configs", exist_ok=True)
    os.makedirs("ragen", exist_ok=True)
    
    trainer = RAGENWebShopTrainer()
    trainer.train()

if __name__ == "__main__":
    main()
