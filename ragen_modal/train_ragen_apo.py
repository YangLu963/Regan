import torch
import torch.optim as optim
import numpy as np
import yaml
import os
from collections import deque
import time

from ragen.qwen_agent import QwenRAGENAgent
from ragen.experience_buffer import ExperienceBuffer
from ragen.webshop_env import WebShopEnv
from ragen.reward_calculator import RewardCalculator

class RAGENWebShopTrainer:
    def __init__(self, config_path="configs/webshop_config.yaml"):
        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        print("=" * 60)
        print("RAGEN + A*PO + Qwen WebShop è®­ç»ƒç³»ç»Ÿ")
        print("=" * 60)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.env = WebShopEnv(
            server_url=self.config['environment']['server_url'],
            max_steps=self.config['environment']['max_steps']
        )
        
        self.agent = QwenRAGENAgent(
            model_name=self.config['model']['base_model'],
            device=self.config['model']['device']
        )
        
        self.reward_calculator = RewardCalculator()
        self.optimizer = optim.Adam(self.agent.parameters(), lr=1e-5)  # å°å­¦ä¹ ç‡
        self.buffer = ExperienceBuffer(1000)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = deque(maxlen=20)
        self.success_rates = deque(maxlen=20)
        self.format_success_rates = deque(maxlen=20)
        self.best_success_rate = 0.0
        self.total_steps = 0
        
    def collect_experience(self, num_episodes=2):
        """æ”¶é›†ç»éªŒæ•°æ®"""
        print(f"\nğŸ“¥ æ”¶é›† {num_episodes} ä¸ªå›åˆçš„ç»éªŒ...")
        
        for episode in range(num_episodes):
            try:
                obs, info = self.env.reset()
                instruction = info['instruction']
                episode_reward = 0
                done = False
                steps = 0
                
                print(f"\n--- å›åˆ {episode+1} ---")
                print(f"ä»»åŠ¡: {instruction}")
                
                while not done and steps < self.config['environment']['max_steps']:
                    # ç”Ÿæˆæ€è€ƒå’ŒåŠ¨ä½œ
                    think_content, action_content, log_prob, full_response = self.agent.generate_webshop_response(obs, instruction)
                    
                    print(f"\næ­¥éª¤ {steps+1}:")
                    print(f"æ€è€ƒ: {think_content}")
                    print(f"åŠ¨ä½œ: {action_content}")
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    next_obs, env_reward, done, info = self.env.step(action_content, info['session_id'])
                    
                    # è®¡ç®—å¥–åŠ±
                    task_success = (env_reward > 0.5)
                    reward = self.reward_calculator.calculate_reward(think_content, action_content, next_obs, task_success)
                    
                    episode_reward += reward
                    steps += 1
                    self.total_steps += 1
                    
                    # å­˜å‚¨ç»éªŒ
                    self.buffer.push(obs, instruction, think_content, action_content, reward, done, log_prob)
                    
                    obs = next_obs
                    
                    if done:
                        break
                
                # è®°å½•ç»Ÿè®¡
                self.episode_rewards.append(episode_reward)
                success = 1 if episode_reward > 0.8 else 0
                self.success_rates.append(success)
                
                format_success = 1 if self._check_format_success(think_content, action_content) else 0
                self.format_success_rates.append(format_success)
                
                current_success = np.mean(self.success_rates) if self.success_rates else 0
                current_format = np.mean(self.format_success_rates) if self.format_success_rates else 0
                
                print(f"\nå›åˆç»“æœ: æ€»å¥–åŠ±={episode_reward:.2f}, æˆåŠŸç‡={current_success:.3f}, æ ¼å¼æˆåŠŸç‡={current_format:.3f}")
                
            except Exception as e:
                print(f"å›åˆ {episode+1} å‡ºé”™: {e}")
                continue
    
    def _check_format_success(self, think_content, action_content):
        """æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®"""
        has_think = think_content and len(think_content) > 5
        has_action = action_content and any(x in action_content for x in ['search[', 'click[', 'buy['])
        return has_think and has_action
    
    def train_step(self):
        """ç®€åŒ–çš„è®­ç»ƒæ­¥éª¤"""
        if len(self.buffer) < 4:  # å°æ‰¹é‡
            return None
            
        batch = self.buffer.sample(4)
        if batch is None:
            return None
        
        # è®¡ç®—ä¼˜åŠ¿ï¼ˆç®€åŒ–ç‰ˆï¼‰
        rewards = torch.FloatTensor(batch['rewards'])
        advantages = rewards - rewards.mean()
        
        # ç­–ç•¥æŸå¤±
        log_probs = torch.FloatTensor(batch['log_probs'])
        policy_loss = -(log_probs * advantages).mean()
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        policy_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.agent.parameters(), 1.0)
        self.optimizer.step()
        
        return {
            'total_loss': policy_loss.item(),
            'avg_reward': rewards.mean().item(),
            'avg_advantage': advantages.mean().item()
        }
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
        print("æˆåŠŸæ ‡å‡†: æˆåŠŸç‡ä»0%æå‡åˆ°20%+")
        print("é‡ç‚¹è§‚å¯Ÿ: Base Modelå­¦ä¹ æ ¼å¼éµå¾ªèƒ½åŠ›")
        print("-" * 50)
        
        start_time = time.time()
        
        for epoch in range(50):  # å‡å°‘epochæ•°
            # æ”¶é›†ç»éªŒ
            self.collect_experience(num_episodes=2)
            
            # è®­ç»ƒ
            if len(self.buffer) >= 4:
                loss_info = self.train_step()
                
                if loss_info:
                    current_success = np.mean(self.success_rates) if self.success_rates else 0
                    current_format = np.mean(self.format_success_rates) if self.format_success_rates else 0
                    
                    print(f"Epoch {epoch:3d} | Loss: {loss_info['total_loss']:7.4f} | "
                          f"Reward: {loss_info['avg_reward']:5.3f} | "
                          f"Success: {current_success:5.3f} | Format: {current_format:5.3f}")
            
            # è¯„ä¼°
            if epoch % 5 == 0:
                current_success = np.mean(self.success_rates) if self.success_rates else 0
                current_format = np.mean(self.format_success_rates) if self.format_success_rates else 0
                
                if current_success > self.best_success_rate:
                    self.best_success_rate = current_success
                
                print(f"\n=== è¯„ä¼° Epoch {epoch} ===")
                print(f"å½“å‰æˆåŠŸç‡: {current_success:.3f}")
                print(f"æ ¼å¼æˆåŠŸç‡: {current_format:.3f}")
                print(f"å†å²æœ€ä½³: {self.best_success_rate:.3f}")
                
                # æˆåŠŸæ ‡å‡†æ£€æŸ¥
                if current_success >= 0.20:
                    print("ğŸ‰ è¾¾åˆ°Part 2ä½œä¸šè¦æ±‚: æˆåŠŸç‡ > 20%!")
                    break
                    
                print("-" * 40)
        
        # æœ€ç»ˆç»Ÿè®¡
        total_time = (time.time() - start_time) / 60
        final_success = np.mean(self.success_rates) if self.success_rates else 0
        
        print(f"\n" + "=" * 50)
        print("è®­ç»ƒå®Œæˆ!")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f} åˆ†é’Ÿ")
        print(f"æœ€ç»ˆæˆåŠŸç‡: {final_success:.3f}")
        print(f"å†å²æœ€ä½³æˆåŠŸç‡: {self.best_success_rate:.3f}")
        print("=" * 50)

def main():
    os.makedirs("configs", exist_ok=True)
    os.makedirs("ragen", exist_ok=True)
    
    trainer = RAGENWebShopTrainer()
    trainer.train()

if __name__ == "__main__":
    main()
